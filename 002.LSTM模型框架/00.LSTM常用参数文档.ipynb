{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM：\n",
    "(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, **kwargs)\n",
    "# Dense\n",
    "(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
    "* Dense就是常用的全连接层，所实现的运算是output = activation(dot(input, kernel)+bias)。其中activation是逐元素计算的激活函数，kernel````是本层的权值矩阵，bias为偏置向量，只有当use_bias=True```才会添加。\n",
    "\n",
    "\n",
    "# 参数详解\n",
    "* units: 正整数，输出空间的维数。\n",
    "* activation：\n",
    "    - tanh：双曲正切激活函数，默认。\n",
    "    - softmax：在多分类中常用的激活函数，是基于逻辑回归的。\n",
    "    - elu：\n",
    "    - selu：\n",
    "    - softplus：softplus(x)=log(1+e^x)，近似生物神经激活函数。\n",
    "    - softsign： \n",
    "    - relu：近似生物神经激活函数。\n",
    "    - sigmoid：S型曲线激活函数。\n",
    "    - hard_sigmoid：基于S型激活函数。\n",
    "    - linear：线性激活函数。\n",
    "    - serialize：\n",
    "    - deserialize：\n",
    "* recurrent_activation： 为循环步施加的激活函数（同activation）\n",
    "* use_bias：布尔值，是否使用偏置项。默认True\n",
    "* kernel_initializer：权值初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。\n",
    "* recurrent_initializer：循环核的初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。（同kernel_initializer）\n",
    "* bias_initializer：权值初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。\n",
    "    - Zeros：全零初始化\n",
    "    - Ones：全1初始化\n",
    "    - Constant：初始化为固定值  - value  keras.initializers.Constant(value=0)\n",
    "    - RandomNormal：正态分布初始化 - keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None))\n",
    "    - RandomUniform：均匀分布初始化 minval：均匀分布下边界 maxval：均匀分布上边界 - keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "    - TruncatedNormal：截尾高斯分布初始化，该初始化方法与RandomNormal类似，但位于均值两个标准差以外的数据将会被丢弃并重新生成，形成截尾分布。该分布是神经网络权重和滤波器的推荐初始化方法。 - keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "    - VarianceScaling：该初始化方法能够自适应目标张量的shape。\n",
    "    - Orthogonal：用随机正交矩阵初始化  gain: 正交矩阵的乘性系数 - keras.initializers.Orthogonal(gain=1.0, seed=None)\n",
    "    - Identiy：使用单位矩阵初始化，仅适用于2D方阵。\n",
    "    - lecun_normal ：LeCun正态分布初始化方法，参数由0均值，标准差为stddev = sqrt(1 / fan_in)的正态分布产生，其中fan_in和fan_out是权重张量的扇入扇出（即输入和输出单元数目）\n",
    "    - lecun_uniform：LeCun均匀分布初始化方法，参数由[-limit, limit]的区间中均匀采样获得，其中limit=sqrt(3 / fan_in), fin_in是权重向量的输入单元数（扇入）\n",
    "    - glorot_normal：Glorot正态分布初始化方法，也称作Xavier正态分布初始化，参数由0均值，标准差为sqrt(2 / (fan_in + fan_out))的正态分布产生，其中fan_in和fan_out是权重张量的扇入扇出（即输入和输出单元数目）\n",
    "    - glorot_uniform：Glorot均匀分布初始化方法，又成Xavier均匀初始化，参数从[-limit, limit]的均匀分布产生，其中limit为sqrt(6 / (fan_in + fan_out))。fan_in为权值张量的输入单元数，fan_out是权重张量的输出单元数。\n",
    "    - he_normal：He正态分布初始化方法，参数由0均值，标准差为sqrt(2 / fan_in) 的正态分布产生，其中fan_in权重张量的扇入\n",
    "    - he_uniform：LeCun均匀分布初始化方法，参数由[-limit, limit]的区间中均匀采样获得，其中limit=sqrt(6 / fan_in), fin_in是权重向量的输入单元数（扇入）\n",
    "* kernel_regularizer：施加在权重上的正则项。\n",
    "* bias_regularizer：施加在偏置向量上的正则项。\n",
    "* recurrent_regularizer：施加在循环核上的正则项。\n",
    "* activity_regularizer：施加在输出上的正则项。\n",
    "    - 为Regularizer对象，需要引用from keras import regularizers\n",
    "    - keras.regularizers.l1(0.)\n",
    "    - keras.regularizers.l2(0.)\n",
    "    - keras.regularizers.l1_l2(0.)\n",
    "* kernel_constraints：施加在权重上的约束项。\n",
    "* recurrent_constraints：施加在循环核上的约束项。\n",
    "* bias_constraints：施加在偏置上的约束项。\n",
    "    - 为Constraints对象，需要引用from keras.constraints import 对应包\n",
    "    - max_norm(m=2)：最大模约束\n",
    "    - non_neg()：非负性约束\n",
    "    - unit_norm()：单位范数约束, 强制矩阵沿最后一个轴拥有单位范数\n",
    "    - min_max_norm(min_value=0.0, max_value=1.0, rate=1.0, axis=0): 最小/最大范数约束\n",
    "* dropout：0~1之间的浮点数，控制输入线性变换的神经元断开比例\n",
    "* recurrent_dropout：0~1之间的浮点数，控制循环状态的线性变换的神经元断开比例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化器optimizers\n",
    "* (optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None, **kwargs)\n",
    "# 参数详解\n",
    "* optimizer：优化器，为预定义优化器名或优化器对象\n",
    "    - SGD：随机梯度下降法，支持动量参数，支持学习衰减率，支持Nesterov动量 \n",
    "        - keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "        - lr：大或等于0的浮点数，学习率\n",
    "        - momentum：大或等于0的浮点数，动量参数\n",
    "        - decay：大或等于0的浮点数，每次更新后的学习率衰减值\n",
    "        - nesterov：布尔值，确定是否使用Nesterov动量\n",
    "    - RMSprop：除学习率可调整外，建议保持优化器的其他默认参数不变，该优化器通常是面对递归神经网络时的一个良好选择\n",
    "        - keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)\n",
    "        - lr：大或等于0的浮点数，学习率\n",
    "        - rho：大或等于0的浮点数\n",
    "        - epsilon：大或等于0的小浮点数，防止除0错误\n",
    "    - Adagrad：建议保持优化器的默认参数不变\n",
    "        - keras.optimizers.Adagrad(lr=0.01, epsilon=1e-06)\n",
    "        - lr：大或等于0的浮点数，学习率\n",
    "        - epsilon：大或等于0的小浮点数，防止除0错误\n",
    "    - Adadelta：建议保持优化器的默认参数不变\n",
    "        - keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-06)\n",
    "        - lr：大或等于0的浮点数，学习率\n",
    "        - rho：大或等于0的浮点数\n",
    "        - epsilon：大或等于0的小浮点数，防止除0错误\n",
    "    - Adam：梯度下降扩展式\n",
    "        - keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "        - lr：大或等于0的浮点数，学习率\n",
    "        - beta_1/beta_2：浮点数， 0<beta<1，通常很接近1\n",
    "        - epsilon：大或等于0的小浮点数，防止除0错误\n",
    "    - Adamax：基于无穷范数的Adam方法的变体\n",
    "        - keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "        - lr：大或等于0的浮点数，学习率\n",
    "        - beta_1/beta_2：浮点数， 0<beta<1，通常很接近1\n",
    "        - epsilon：大或等于0的小浮点数，防止除0错误\n",
    "    - Nadam：Adam本质上像是带有动量项的RMSprop，Nadam就是带有Nesterov 动量的Adam RMSprop默认参数来自于论文，推荐不要对默认参数进行更改\n",
    "        - keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "        - lr：大或等于0的浮点数，学习率\n",
    "        - beta_1/beta_2：浮点数， 0<beta<1，通常很接近1\n",
    "        - epsilon：大或等于0的小浮点数，防止除0错误\n",
    "* loss：损失函数，为预定义损失函数名或一个目标函数 \n",
    "    - 需要引用from keras import losses\n",
    "    - mean_squared_error或mse\n",
    "    - mean_absolute_error或mae\n",
    "    - mean_absolute_percentage_error或mape\n",
    "    - mean_squared_logarithmic_error或msle\n",
    "    - squared_hinge\n",
    "    - hinge\n",
    "    - categorical_hinge\n",
    "    - binary_crossentropy（亦称作对数损失，logloss）\n",
    "    - logcosh\n",
    "    - categorical_crossentropy：亦称作多类的对数损失，注意使用该目标函数时，需要将标签转化为形如(nb_samples, nb_classes)的二值序列\n",
    "    - sparse_categorical_crossentrop：如上，但接受稀疏标签。注意，使用该函数时仍然需要你的标签与输出值的维度相同，你可能需要在标签数据上增加一个维度：np.expand_dims(y,-1)\n",
    "    - kullback_leibler_divergence:从预测值概率分布Q到真值概率分布P的信息增益,用以度量两个分布的差异.\n",
    "    - poisson：即(predictions - targets * log(predictions))的均值\n",
    "    - cosine_proximity：即预测值与真实标签的余弦距离平均值的相反数\n",
    "* metrics：列表，包含评估模型在训练和测试时的性能的指标，典型用法是metrics=['accuracy']如果要在多输出模型中为不同的输出指定不同的指标，可像该参数传递一个字典，例如metrics={'ouput_a': 'accuracy'}\n",
    "    - 需要引用 from keras import metrics\n",
    "    - binary_accuracy: 对二分类问题,计算在所有预测值上的平均正确率\n",
    "    - categorical_accuracy:对多分类问题,计算再所有预测值上的平均正确率\n",
    "    - sparse_categorical_accuracy:与categorical_accuracy相同,在对稀疏的目标值预测时有用\n",
    "    - top_k_categorical_accracy: 计算top-k正确率,当预测值的前k个值中存在目标类别即认为预测正确\n",
    "    - sparse_top_k_categorical_accuracy：与top_k_categorical_accracy作用相同，但适用于稀疏情况\n",
    "    - mse = MSE = mean_squared_error\n",
    "    - mae = MAE = mean_absolute_error\n",
    "    - mape = MAPE = mean_absolute_percentage_error\n",
    "    - msle = MSLE = mean_squared_logarithmic_error\n",
    "    - cosine = cosine_proximity\n",
    " \n",
    " \n",
    "* 无须调节\n",
    " \n",
    "\n",
    "* sample_weight_mode：如果你需要按时间步为样本赋权（2D权矩阵），将该值设为“temporal”。默认为“None”，代表按样本赋权（1D权）。如果模型有多个输出，可以向该参数传入指定sample_weight_mode的字典或列表。参考fit函数。\n",
    "* weighted_metrics: metrics列表，在训练和测试过程中，这些metrics将由sample_weight或clss_weight计算并赋权\n",
    "* target_tensors: 默认情况下，Keras将为模型的目标创建一个占位符，该占位符在训练过程中将被目标数据代替。如果你想使用自己的目标张量（相应的，Keras将不会在训练时期望为这些目标张量载入外部的numpy数据），你可以通过该参数手动指定。目标张量可以是一个单独的张量（对应于单输出模型），也可以是一个张量列表，或者一个name->tensor的张量字典。\n",
    "* kwargs：使用TensorFlow作为后端请忽略该参数，若使用Theano/CNTK作为后端，kwargs的值将会传递给 K.function。如果使用TensorFlow为后端，这里的值会被传给tf.Session.run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit\n",
    "(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, **kwargs)\n",
    "# 参数详解\n",
    "- x：输入数据。如果模型只有一个输入，那么x的类型是numpy array，如果模型有多个输入，那么x的类型应当为list，list的元素是对应于各个输入的numpy array。如果模型的每个输入都有名字，则可以传入一个字典，将输入名与其输入数据对应起来。\n",
    "- y：标签，numpy array。如果模型有多个输出，可以传入一个numpy array的list。如果模型的输出拥有名字，则可以传入一个字典，将输出名与其标签对应。\n",
    "- batch_size：整数，指定进行梯度下降时每个batch包含的样本数。训练时一个batch的样本会被计算一次梯度下降，使目标函数优化一步。\n",
    "- epochs：整数，训练终止时的epoch值，训练将在达到该epoch值时停止，当没有设置initial_epoch时，它就是训练的总轮数，否则训练的总轮数为epochs - inital_epoch\n",
    "- verbose：日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录，2为每个epoch输出一行记录\n",
    "- callbacks：list，其中的元素是keras.callbacks.Callback的对象。这个list中的回调函数将会在训练过程中的适当时机被调用，参考回调函数\n",
    "- validation_split：0~1之间的浮点数，用来指定训练集的一定比例数据作为验证集。验证集将不参与训练，并在每个epoch结束后测试的模型的指标，如损失函数、精确度等。注意，validation_split的划分在shuffle之后，因此如果你的数据本身是有序的，需要先手工打乱再指定validation_split，否则可能会出现验证集样本不均匀。\n",
    "- validation_data：形式为（X，y）或（X，y，sample_weights）的tuple，是指定的验证集。此参数将覆盖validation_spilt。\n",
    "- shuffle：布尔值，表示是否在训练过程中每个epoch前随机打乱输入样本的顺序。\n",
    "- class_weight：字典，将不同的类别映射为不同的权值，该参数用来在训练过程中调整损失函数（只能用于训练）。该参数在处理非平衡的训练数据（某些类的训练样本数很少）时，可以使得损失函数对样本数不足的数据更加关注。\n",
    "- sample_weight：权值的numpy array，用于在训练时调整损失函数（仅用于训练）。可以传递一个1D的与样本等长的向量用于对样本进行1对1的加权，或者在面对时序数据时，传递一个的形式为（samples，sequence_length）的矩阵来为每个时间步上的样本赋不同的权。这种情况下请确定在编译模型时添加了sample_weight_mode='temporal'。\n",
    "- initial_epoch: 从该参数指定的epoch开始训练，在继续之前的训练时有用。\n",
    "- steps_per_epoch: 一个epoch包含的步数（每一步是一个batch的数据送入），当使用如TensorFlow数据Tensor之类的输入张量进行训练时，默认的None代表自动分割，即数据集样本数/batch样本数。\n",
    "- validation_steps: 仅当steps_per_epoch被指定时有用，在验证集上的step总数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
